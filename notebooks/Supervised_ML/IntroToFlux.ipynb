{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook on Supervised Machine Learning\n",
    "---\n",
    "\n",
    "**Flux is a 100% pure Julia library** for deep learning, that is increasingly very capable.\n",
    "\n",
    "Most other popular libraries are in Python, though the numerically intensive parts are written in C, C++ or CUDA.\n",
    "\n",
    "Flux keeps it simple.\n",
    "\n",
    "The whole library is written in a language that is understandable and minimal confusion from boilerplate code that acts as an \"interaction module\" between two different languages.\n",
    "\n",
    "This makes Flux and extremely powerful platform to innovate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux internally calls powerful automatic differentiation (AD) libraries such Zygote and ChainRules that enable differentiation of arbitrary functions.\n",
    "\n",
    "As an example, lets define a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 3x^2 + 2x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute a derivative of the function using Flux.gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x) = Flux.gradient(f,x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute a 2nd derivative of the same function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2f(x) = Flux.gradient(df,x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2f(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Flux internally imports the *gradient* function from Zygote. So if we want more customized gradient behavior, we can write it directly in Zygote and use it in Flux code. It will play well seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x,W,b) = sum(W*x + b)\n",
    "\n",
    "x = rand(5)\n",
    "W = randn(3,5)\n",
    "b = rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x,W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined a loss function, and some \"dummy\" input x, with parameters W and b.\n",
    "We can compute the gradients of the parameters with respect to loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(W*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Flux.gradient(loss, x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the gradient computation gives a Tuple of size 3, with the derivative of the loss with respect to each \n",
    "variable.\n",
    "\n",
    "Since this is explicit passing of arguments, this can be cumbersome for neural networks that have a large number of parameters. Flux allows us to abstract this away with the *params* function.\n",
    "\n",
    "Here, the *params* automatically extracts the parameters in a model and implicitly passes it to the *gradient* function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x) = sum(W*x .+ b)\n",
    "grads = Flux.gradient(()->model(x), Flux.params([W, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the rest of the training workflow. We generate some \"dummy\" data for ground truth $\\hat{y}$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "天 = rand(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define an optimizer: Flux has several in-built optimizers. Here, we use the classic Gradient Descent algorithm, with a learning rate $\\alpha = 0.1$ passed as an argument.\n",
    "\n",
    "More optimizers, such as ADAM and AdaDelta are available. More details: https://fluxml.ai/Flux.jl/stable/training/optimisers/#Optimiser-Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Flux.Descent(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a loss function manually to demonstrate the flexibility (since scientific ML often uses custom loss functions). However, Flux has its own predefined set of commonly used losses, such as MSE. More details here: https://fluxml.ai/Flux.jl/stable/models/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(x, 天)\n",
    "  y = model(x)\n",
    "  sum((y .- 天).^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the supervised learning training dataset pair with $x$ and $\\hat{y}$ and train with the *Flux.train* function. Each call of this function performs gradient descent and optimization for just 1 epoch.\n",
    "\n",
    "We can put this function in a for loop or use the *@epochs* macro - which we will see in a complete example in the MNIST notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(x,天)\n",
    "\n",
    "Flux.train!(loss, Flux.params(model), data, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux Layers\n",
    "\n",
    "Apart from AD, an integral part of a good ML framework is predefined layers. We look at some of them now, with other notebooks to demonstrate it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense layers\n",
    "\n",
    "The classic, fully-connected neural network layer. Flux also provides ready access to various activation functions.\n",
    "\n",
    "**Note**: Lot of the \"primitive\" operations are provided by Julia NNlib.jl library, which is part of FluxML family and is also 100% pure Julia. It has activation functions, primitive convolution and other helper functions. Flux uses these to build higher level operations, such as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Flux.Dense(5, 5, tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 Weight params + 5 bias params = 30 total trainable parameters\n",
    "\n",
    "We can also define a vector of layers, with each element containing a Dense layer and/or activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Flux.Dense(5, 10, Flux.sigmoid), Flux.Dense(10, 2), Flux.softmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass arguments through each element to compute the layer output.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = layers[1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = layers[2](out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = layers[3](out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not the only way to do multiple layers. In practice, we use Flux built-in *Chain* utility that packages these layers such that the outputs of each feed into the next layers as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Flux.Chain(Flux.Dense(5, 10, Flux.sigmoid), Flux.Dense(10, 2), Flux.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_chain = m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux provides Convolutional Layers where we can set:\n",
    "* Kernel size \n",
    "* Channels \n",
    "* Padding \n",
    "* Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convlayer = Flux.Conv((3,3), 1 => 1, pad=1, Flux.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel size = 3 x 3\n",
    "\n",
    "Input channels = 1\n",
    "\n",
    "output channels = 1\n",
    "\n",
    "padding for domain = 1\n",
    "\n",
    "Activation function = ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatrix = Float32.(randn(10,10,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_conv = convlayer(xmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common issue: matrices are default Float64, while Flux layers use default Float32. This can cause inefficiences in training (and Flux will caution you). Make sure to explicitly cast type"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fabbfb9292bb03a077b67a9767abe5157b7697c4659fb99b687d3ca946780012"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
