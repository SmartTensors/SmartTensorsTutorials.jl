{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fad8cc4",
   "metadata": {},
   "source": [
    "Notebook on simple multilayer perceptron or artificial neural network\n",
    "=\n",
    "More examples can be found here [MLmodels](https://github.com/FluxML/model-zoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42791340",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux.Data: DataLoader\n",
    "using Flux: onehotbatch, onecold, @epochs\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using Base: @kwdef\n",
    "using CUDA\n",
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83141094",
   "metadata": {},
   "source": [
    "## Data loader\n",
    "For every ml model, we need data\n",
    "\n",
    "We can either generate synthetic data or load data from file\n",
    "\n",
    "Here, we will load data from file of MNIST image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "function getdata(args, device)\n",
    "    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n",
    "\n",
    "    # Loading Dataset\t\n",
    "    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)\n",
    "    xtest, ytest = MLDatasets.MNIST.testdata(Float32)\n",
    "\t\n",
    "    # Reshape Data in order to flatten each image into a linear array\n",
    "    xtrain = Flux.flatten(xtrain)\n",
    "    xtest = Flux.flatten(xtest)\n",
    "\n",
    "    # One-hot-encode the labels\n",
    "    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n",
    "\n",
    "    # Create DataLoaders (mini-batch iterators)\n",
    "    train_loader = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=true)\n",
    "    test_loader = DataLoader((xtest, ytest), batchsize=args.batchsize)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49319229",
   "metadata": {},
   "source": [
    "## Build a neural network model\n",
    "Next thing is creating a neural network model\n",
    "\n",
    "Here, we will build a neural network consisting two layers each containig 32 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model(; imgsize=(28,28,1), nclasses=10)\n",
    "    return Chain( Dense(prod(imgsize), 32, relu),\n",
    "                  Dense(32, nclasses))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5085f4",
   "metadata": {},
   "source": [
    "## Define loss and accuracy functions\n",
    "Next step is defining loss ($\\mathcal{L}$) and accuracy functions\n",
    "\n",
    "Loss function computes the accuracy by comparing true values versus machine learning predicted values based on a specified function.\n",
    "\n",
    "Here, we used logitcrossentropy function, which is good for classification\n",
    "\n",
    "Accuracy function computes the accuracy by comparing true values versus machine learning predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_and_accuracy(data_loader, model, device)\n",
    "    acc = 0\n",
    "    ls = 0.0f0\n",
    "    num = 0\n",
    "    for (x, y) in data_loader\n",
    "        x, y = device(x), device(y)\n",
    "        ŷ = model(x)\n",
    "        ls += logitcrossentropy(ŷ, y, agg=sum)\n",
    "        acc += sum(onecold(ŷ) .== onecold(y))\n",
    "        num +=  size(x)[end]\n",
    "    end\n",
    "    return ls / num, acc / num\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0a7b2",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "Neural network has often several parameters\n",
    "\n",
    "Tuning those parameters help improving the model performance\n",
    "\n",
    "Values of those parameters may vary widely, so it is better to specify them\n",
    "\n",
    "Here, we are specifying three hyperparameters with just one value; however, those values can be changed during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kwdef mutable struct Args\n",
    "    η::Float64 = 3e-4       # learning rate\n",
    "    batchsize::Int = 256    # batch size\n",
    "    epochs::Int = 10        # number of epochs\n",
    "    use_cuda::Bool = true   # use gpu (if cuda available)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3d06e",
   "metadata": {},
   "source": [
    "## Build a training function\n",
    "We generated ML model using neural network, defined loss and accuracy functions, and also defined some hyperparameters to tune\n",
    "\n",
    "Now, we need to utilize them and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09cd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(; kws...)\n",
    "    args = Args(; kws...) # collect options in a struct for convenience\n",
    "\n",
    "    if CUDA.functional() && args.use_cuda\n",
    "        @info \"Training on CUDA GPU\"\n",
    "        CUDA.allowscalar(false)\n",
    "        device = gpu\n",
    "    else\n",
    "        @info \"Training on CPU\"\n",
    "        device = cpu\n",
    "    end\n",
    "\n",
    "    # Create test and train dataloaders\n",
    "    train_loader, test_loader = getdata(args, device)\n",
    "\n",
    "    # Construct model\n",
    "    model = build_model() |> device\n",
    "    ps = Flux.params(model) # model's trainable parameters\n",
    "    \n",
    "    ## Optimizer\n",
    "    opt = ADAM(args.η)\n",
    "    \n",
    "    ## Training\n",
    "    for epoch in 1:args.epochs\n",
    "        for (x, y) in train_loader\n",
    "            x, y = device(x), device(y) # transfer data to device\n",
    "            gs = gradient(() -> logitcrossentropy(model(x), y), ps) # compute gradient\n",
    "            Flux.Optimise.update!(opt, ps, gs) # update parameters\n",
    "        end\n",
    "        \n",
    "        # Report on train and test\n",
    "        train_loss, train_acc = loss_and_accuracy(train_loader, model, device)\n",
    "        test_loss, test_acc = loss_and_accuracy(test_loader, model, device)\n",
    "        println(\"Epoch=$epoch\")\n",
    "        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run training \n",
    "if abspath(PROGRAM_FILE) == @__FILE__\n",
    "    train()\n",
    "end\n",
    "# train(η=0.01) # can change hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(η=0.01) # changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bba188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
