{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Julia GPU Acceleation\n",
    "\n",
    "Julia provides excellent native GPU support.\n",
    "\n",
    "GPUs are devices which can run thousands of threads simultanously in parallel.\n",
    "\n",
    "GPU threads are slower and memory limited than the CPU threads.\n",
    "\n",
    "However, there are so many of GPU threads.  \n",
    "\n",
    "Many tasks can be executed much faster on a GPU than on a CPU, if these tasks can be parallelized.\n",
    "\n",
    "## Check installed GPU device:"
   ],
   "metadata": {
    "id": "nUl0Nj9dV-IW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    ";nvidia-smi"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTKc9IFB-eum",
    "outputId": "25fb764d-7fa5-4763-a454-732f4fafd0b4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ULS7Jn6vws2-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Banchmark CPU performace\n",
    "\n",
    "Let us create a large matrix and time how long it takes to square it on the CPU:"
   ],
   "metadata": {
    "id": "HzVnsDN--gRr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import BenchmarkTools\n",
    "\n",
    "M = rand(2^11, 2^11)\n",
    "\n",
    "function benchmark_matmul_cpu(M)\n",
    "    M * M\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "benchmark_matmul_cpu(M) # warm up\n",
    "\n",
    "@BenchmarkTools.btime benchmark_matmul_cpu($M)\n",
    "@BenchmarkTools.benchmark benchmark_matmul_cpu($M)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rbfON6gWYwe",
    "outputId": "51246720-8aca-41eb-e28e-4910945a4c9c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For benchmarking, the math operations are in a function which returns `nothing`.\n",
    "  \n",
    "We need a \"warm up\" line.\n",
    "Since Julia compiles code on the fly the first time it is executed, the operation we want to benchmark needs to be exectured at least once before starting the benchmark\n",
    "Otherwise, the benchmark will include the compilation time.\n",
    "\n",
    "Note that `$M` is used instead of `M` for banchmarking.\n",
    "This is a feature of the `@BenchmarkTools.btime` macro.\n",
    "It allows evaluation of `M` before benchmarking takes place, to avoid the extra delay that is incurred when benchmarking with global variables."
   ],
   "metadata": {
    "id": "LRO3dOh1lnVg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Banchmark GPU performace\n",
    "\n",
    "GPU operations using CUDA: "
   ],
   "metadata": {
    "id": "x-nbiuYDtGlb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import CUDA\n",
    "\n",
    "M_on_gpu = CUDA.cu(M) # Copy the data to the GPU and create a CuArray\n",
    "\n",
    "M_on_gpu = CUDA.CURAND.rand(size(M)) # or create a new random matrix directly on the GPU\n",
    "\n",
    "function benchmark_matmul_gpu(M)\n",
    "    CUDA.@sync M * M\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "benchmark_matmul_gpu(M_on_gpu) # warm up\n",
    "\n",
    "@BenchmarkTools.btime benchmark_matmul_gpu($M_on_gpu)\n",
    "@BenchmarkTools.benchmark benchmark_matmul_gpu($M_on_gpu)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hIWY1O_WE6a",
    "outputId": "68f65877-efb5-4b3c-c4b6-cbea8cee2dc7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important:\n",
    "* Before the GPU can work on some data, it needs to be copied to the GPU (or generated there directly).\n",
    "* The `CUDA.@sync` macro waits for the GPU operation to complete.\n",
    "  Without it, the operation would happen in parallel on the GPU, while execution would continue on the CPU.\n",
    "  So we would just be timing how long it takes to _start_ the operation, not how long it takes to complete.\n",
    "* In general, you don't need `CUDA.@sync`, since many operations (including `cu()`) call it implicitly.\n",
    "  And it is usually a good idea to let the CPU and GPU work in parallel.\n",
    "  Typically, the GPU will be working on the current batch of data while the CPU works on preparing the next batch."
   ],
   "metadata": {
    "id": "EbjxRKWc1-nR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU memory status"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check how much RAM we have left on the GPU:"
   ],
   "metadata": {
    "id": "WdvtwJAu3aOU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CUDA.memory_status()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "599aiP0s3c4y",
    "outputId": "ceca46a8-5d45-4b39-87d5-07109f45e439"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Julia's Garbage Collector (GC) will free CUDA arrays like any other object, when there's no more reference to it.\n",
    "\n",
    "However, `CUDA.jl` uses a memory pool to make allocations faster on the GPU, so don't be surprised if the allocated memory on the GPU does not go down immediately.\n",
    "\n",
    "Moreover, IJulia keeps a reference to the output of each cell, so if you let any cell output a `CuArray` it will be staying in the memory.\n",
    "\n",
    "To force the Garbage Collector to run, execute `GC.gc()`. To reclaim memory from the memory pool, use `CUDA.reclaim()`:"
   ],
   "metadata": {
    "id": "86XHVDDrDJjp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "GC.gc()\n",
    "CUDA.reclaim()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPoO6-h_tVBE",
    "outputId": "2cb4aacb-6397-4f1a-9645-ad2587151f6d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU loop fussion\n",
    "\n",
    "Many other operations are implemented for `CuArray` (`+`,  `-`, etc.).\n",
    "\n",
    "Their dotted versions are implemented as well (`.+`, `exp.()`, etc).\n",
    "\n",
    "Importantly, the Julia loop fusion also works on the GPU.\n",
    "\n",
    "For example, if we want to compute `M .* M .+ M`, without loop fusion the GPU would first compute `M .* M` and create a temporary array, then it would add `M` to that array, like this:"
   ],
   "metadata": {
    "id": "ndcNywPSE0D9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "function benchmark_without_fusion(M)\n",
    "    P = M .* M\n",
    "    CUDA.@sync P .+ M\n",
    "    return\n",
    "end\n",
    "\n",
    "benchmark_without_fusion(M_on_gpu) # warm up\n",
    "\n",
    "@BenchmarkTools.btime benchmark_without_fusion($M_on_gpu)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psC2K-M_F0w9",
    "outputId": "7fd3ea87-ae51-439e-90f2-f99ae3aecda7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead, the loop fusion in Julia ensures that the array is only traversed once, without the need for a temporary array:"
   ],
   "metadata": {
    "id": "DTbDFRdJIO9M"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "function benchmark_with_fusion(M)\n",
    "    CUDA.@sync M .* M .+ M\n",
    "    return\n",
    "end\n",
    "\n",
    "benchmark_with_fusion(M_on_gpu) # warm up\n",
    "\n",
    "@BenchmarkTools.btime benchmark_with_fusion($M_on_gpu)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHQI4nkHGkLx",
    "outputId": "309405b9-18d6-4a4c-8c23-279e542072d9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU coding\n",
    "\n",
    "Julia allows you to write your own GPU operations!\n",
    "\n",
    "Rather than using GPU operations implemented in the `CUDA.jl` package (or others), you can write Julia code that will be compiled for the GPU, and executed there.\n",
    "\n",
    "This can be useful to speed up some algorithms where the standard kernels do not suffice.\n",
    "\n",
    "For example, here's a GPU kernel which implements `u .+= v`, where `u` and `v` are two (large) vectors:"
   ],
   "metadata": {
    "id": "9HAHfceIa3U5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "function worker_gpu_add!(u, v)\n",
    "    index = (CUDA.blockIdx().x - 1) * CUDA.blockDim().x + CUDA.threadIdx().x\n",
    "    index ≤ length(u) && (@inbounds u[index] += v[index])\n",
    "    return\n",
    "end\n",
    "\n",
    "function gpu_add!(u, v)\n",
    "    numblocks = ceil(Int, length(u) / 256)\n",
    "    CUDA.@cuda threads=256 blocks=numblocks worker_gpu_add!(u, v)\n",
    "    return u\n",
    "end"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnbRTzgWX0gx",
    "outputId": "3b1decaa-d578-4be8-dcc7-0c7262697a18"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important:\n",
    "\n",
    "* The `gpu_add!()` function first calculates `numblocks`, the number of blocks of threads to start, then it uses the `CUDA.@cuda` macro to spawn `numblocks` blocks of 256 GPU threads and each thread executeds `worker_gpu_add!(u, v)`.\n",
    "* The `worker_gpu_add!()` function computes `u[index] += v[index]` for a single value of `index`: in other words, each thread will just update a single value in the vector!\n",
    "* The `CUDA.@cuda` macro spawns `numblocks` blocks of 256 threads each.\n",
    "  These blocks are organized in a grid, which is one-dimensional by default, but it can be up to three-dimensional.\n",
    "  Therefore each thread and each block have an `(x, y, z)` coordinate in this grid.\n",
    "  See this diagram from the [Nvidia blog post](https://developer.nvidia.com/blog/even-easier-introduction-cuda/):<br />\n",
    "  <img src=\"https://juliagpu.gitlab.io/CUDA.jl/tutorials/intro1.png\" width=\"600\"/>.\n",
    "* `CUDA.threadIdx().x` returns the current GPU thread's `x` coordinate within its block (one difference with the diagram is that Julia is 1-indexed).\n",
    "* `CUDA.blockIdx().x` returns the current block's `x` coordinate in the grid.\n",
    "* `CUDA.blockDim().x` returns the block size along the `x` axis (in this example, it's 256).\n",
    "* `CUDA.gridDim().x` returns the number of blocks in the grid, along the `x` axis (in this example it's `numblocks`).\n",
    "* So the `index` that each thread must update in the array is `(CUDA.blockIdx().x - 1) * CUDA.blockDim().x + CUDA.threadIdx().x`.\n",
    "* As explained earlier, the `@inbounds` macro is an optimization that tells Julia that the index is guaranteed to be inbounds, so there's no need for it to check.\n",
    "\n",
    "Hopefully, now writing your own GPU kernel in Julia will not seem like something only top experts with advanced C++ skills can do."
   ],
   "metadata": {
    "id": "WEnX1i4ooQiN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check that the kernel works as expected:"
   ],
   "metadata": {
    "id": "gf6-5tJXRCm0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "u = rand(2^20)\n",
    "v = rand(2^20)\n",
    "\n",
    "u_on_gpu = CUDA.cu(u)\n",
    "v_on_gpu = CUDA.cu(v)\n",
    "\n",
    "u .+= v\n",
    "\n",
    "gpu_add!(u_on_gpu, v_on_gpu)\n",
    "\n",
    "@assert Array(u_on_gpu) ≈ u"
   ],
   "outputs": [],
   "metadata": {
    "id": "l7mGsSWYL2iW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: the `≈` operator checks whether the operands are approximately equal within the float precision limit."
   ],
   "metadata": {
    "id": "GCZyhpbwNeLQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us benchmark the new custom kernel:"
   ],
   "metadata": {
    "id": "Koqs9GrqRHeW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "function benchmark_custom_assign_add!(u, v)\n",
    "    CUDA.@sync gpu_add!(u, v)\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "benchmark_custom_assign_add!(u_on_gpu, v_on_gpu) # warm up\n",
    "\n",
    "@BenchmarkTools.btime benchmark_custom_assign_add!($u_on_gpu, $v_on_gpu)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2_7SawNR4Jb",
    "outputId": "9c50e0a2-d23d-4ba4-9ed2-d8b5f0aec819"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us see how this compares to `CUDA.jl`'s implementation:"
   ],
   "metadata": {
    "id": "hl0_1Km9SyD7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "function benchmark_assign_add!(u, v)\n",
    "    CUDA.@sync u .+= v\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "benchmark_assign_add!(u_on_gpu, v_on_gpu) # warm up\n",
    "\n",
    "@BenchmarkTools.btime benchmark_assign_add!($u_on_gpu, $v_on_gpu)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vogLKdtEO9l6",
    "outputId": "6a7b197b-2a02-46a1-d660-adcf3b40a414"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The new custom kernel is just as fast as `CUDA.jl`'s kernel!\n",
    "\n",
    "Howvever, the new kernel would not work with huge vectors!\n",
    "\n",
    "This is because there is a limit to the number of blocks & threads can be spawned. \n",
    "\n",
    "To support huge vectors, each worker needs to run a loop like this:"
   ],
   "metadata": {
    "id": "IKRNLPIRcbCd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "function worker_gpu_add!(u, v)\n",
    "    index = (CUDA.blockIdx().x - 1) * CUDA.blockDim().x + CUDA.threadIdx().x\n",
    "    stride = CUDA.blockDim().x * CUDA.gridDim().x\n",
    "    for i = index:stride:length(u)\n",
    "        @inbounds u[i] += v[i]\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zi8-UvGocWOK",
    "outputId": "8ffe3107-5923-452b-bf96-169f746abbef"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This way, if `CUDA.@cuda` is executed with a smaller number of blocks than needed to have one thread per array item, the workers will loop appropriately.\n",
    "\n",
    "For more info, check out [`CUDA.jl`'s documentation](https://juliagpu.gitlab.io/CUDA.jl)."
   ],
   "metadata": {
    "id": "YtQNhlw1c3zo"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Julia_for_Pythonistas.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}